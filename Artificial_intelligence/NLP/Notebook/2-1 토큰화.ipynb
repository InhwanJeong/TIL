{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bd36af2",
   "metadata": {},
   "source": [
    "# 텍스트 전처리\n",
    "자연어 처리는 데이터의 용도에 맞게 다음 세가지 작업이 필요합니다.\n",
    "- 토큰화(tokenization)\n",
    "- 정제(cleaning)\n",
    "- 정규화(normalization)\n",
    "\n",
    "\n",
    "# 1. 토큰화\n",
    "- 주어진 코퍼스(corpus, 말뭉치)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화라고 한다.\n",
    "- 토큰은 단위가 상황에따라 다르며, 의미있는 단위로 토큰을 정의\n",
    "\n",
    "## 1) 단어 토큰화(Word Tokenization)\n",
    "- 토큰의 기준을 단어(word)로 하는 경우 단어 토큰화라고 한다. \n",
    "  - ⚠︎단어는 단어 단위외에도 단어구, 의미를 갖는 문자열로도 간주된다.\n",
    "- 띄어쓰기 구분으로 토큰화 실시\n",
    "  - 구두점이나 특수문자를 전부 제거하면 토큰이 의미를 잃어버리는 경우도 발생\n",
    "\n",
    "➜ 구두점(punctuation): 마침표(.), 컴마(,), 물음표(?), 세미콜론(;), 느낌표(!) 등과 같은 기호\n",
    "\n",
    "\n",
    "## 2) 토큰화 중 생기는 선택의 순간\n",
    "- 아포스트로피(') 예시\n",
    "  - Don't\n",
    "  - Don t\n",
    "  - Dont\n",
    "  - Do n't\n",
    "  - Jone's\n",
    "  - Jone s\n",
    "  - Jone\n",
    "  - Jones\n",
    "  - 위와 같이 아포스트로피가 들어간 상황에서 다양한 선택지가 있다.\n",
    "- NLTK는 영어 코퍼스를 토큰화 하기 위한 도구를 제공\n",
    "  - word_tokenize\n",
    "  - WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83124a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화1 : ['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.'] \n",
      "\n",
      "단어 토큰화2 : ['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.'] \n",
      "\n",
      "단어 토큰화3 : [\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphanage', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "word = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
    "\n",
    "print('단어 토큰화1 :',word_tokenize(word), '\\n')# Don't를 Do와 n't로 분리, Jone과 's로 분리\n",
    "print('단어 토큰화2 :',WordPunctTokenizer().tokenize(word), '\\n')# Don과 '와 t로 분리, Jone, ', t로 분리\n",
    "print('단어 토큰화3 :',text_to_word_sequence(word))# 알파벳을 모두 소문자로 변경 및 아포스트로피 보존, 구두점 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8598e5",
   "metadata": {},
   "source": [
    "## 3) 토큰화에서 고려해야할 사항\n",
    "- 토큰화 작업은 단순히 코퍼스(말뭉치)에서 구두점을 제외하고 공백 기준으로 잘라내는 작업이 아니다.\n",
    "\n",
    "### (1) 구두점이나 특수문자를 단순 제외해서는 안 된다.\n",
    "- 구두점조차 하나의 토큰으로 분류할 때가 있다. 특히 마침표와 같은 경우 문장의 경계를 알 수 있기 때문에 제외하지 않을 수 있다.\n",
    "- 단어 자체에 구두점을 가지고 있을 수 있다. m.p.h, 01/02/06, 45.55, 100,000,000 등\n",
    "\n",
    "### (2) 줄임말과 단어 내에 띄어쓰기가 있는 경우\n",
    "- I'm, What're 등 이러한 단어도 하나로 인식할 수 있는 능력을 가져야 한다.\n",
    "\n",
    "### (3) 표준 토큰화 예제\n",
    "- penn Treebank Tokenization\n",
    "  - 하이푼으로 구성된 단어는 하나로 유지한다.\n",
    "  - doesn't와 같이 아포스트로피로 '접어'가 함께 하는 단어는 분리해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55c48e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리뱅크 워드토크나이저 : ['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal.', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print('트리뱅크 워드토크나이저 :',tokenizer.tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42e588",
   "metadata": {},
   "source": [
    "## 4) 문장 토큰화(Sentence Tokenization)\n",
    "- 문장 분류(sentence segmentation)라고도 부른다.\n",
    "- 직관적으로 마침표 등으로 분리하면 안된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c033903a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화1 : ['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n",
      "문장 토큰화2 : ['I am actively looking for Ph.D. students.', 'and you are a Ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "print('문장 토큰화1 :',sent_tokenize(text))\n",
    "\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print('문장 토큰화2 :',sent_tokenize(text)) # 마침표 다수 등장 - 잘 처리하는 것으로 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1e19760",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kss\n",
      "  Downloading kss-3.7.3.tar.gz (42.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting emoji==1.2.0\n",
      "  Downloading emoji-1.2.0-py3-none-any.whl (131 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex in /Users/inan/opt/anaconda3/envs/tutorial_nlp/lib/python3.10/site-packages (from kss) (2022.10.31)\n",
      "Collecting more_itertools\n",
      "  Downloading more_itertools-9.0.0-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: kss\n",
      "  Building wheel for kss (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kss: filename=kss-3.7.3-py3-none-any.whl size=42449191 sha256=8223be1aa5c0293c87a7fed952e644c75dd7d6d40fab1ed95c7bc1e1f48fd4c1\n",
      "  Stored in directory: /Users/inan/Library/Caches/pip/wheels/6f/df/16/4fe9d6e70725618c8a16f424144d6c0f9dd56ef61a502d0fb9\n",
      "Successfully built kss\n",
      "Installing collected packages: emoji, more_itertools, kss\n",
      "Successfully installed emoji-1.2.0 kss-3.7.3 more_itertools-9.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install kss # 한국어 문장 토큰 도구(박상길님), kss(Korean Sentence Splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2a4f44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어 문장 토큰화 : ['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다.', '이제 해보면 알걸요?', 'Q.E.D']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요? Q.E.D'\n",
    "print('한국어 문장 토큰화 :',kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b18d88",
   "metadata": {},
   "source": [
    "## 5) 한국어에서의 토큰화 어려움\n",
    "- 영어는 띄어쓰기 기준으로 하는 토큰화만 수행해도 잘 동작한다. 하지만 한국어는 띄어쓰기 단위로 나누는걸 지양해야한다. 한국어는 교착어기 때문이다.\n",
    "\n",
    "➜ 교착어: 조사, 어미 등을 붙여서 말을 만드는 언어\n",
    "\n",
    "### (1) 교착어의 특성\n",
    "- 한국어에는 조사가 존재하며, 같은 단어임에도 서로 다른 조사가 붙어서 자연어 처리가 힘들어진다.\n",
    "  - 따라서 한국어 NLP에서 조사는 분리해줄 필요가 있다.\n",
    "\n",
    "➜ 조사: '그'라는 단어 뒤에 그는, 그에게, 그와 등 조사가 많이 붙게된다.\n",
    "\n",
    "---\n",
    "\n",
    "- 한국어 토큰화에서는 형태소(morpheme)란 개념을 반드시 이해해야한다.\n",
    "\n",
    "➜ 형태소: 가장 작은 말의 단위\n",
    "\n",
    "➜ 자립 형태소: 접사, 어미, 조사와 상관없이 자립하여 사용할 수 있는 형태소. 그 자체로 단어가 된다. 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 등이 있다.\n",
    "\n",
    "➜ 의존 형태소: 다른 형태소와 결합하여 사용되는 형태소. 접사, 어미, 조사, 어간\n",
    "\n",
    "---\n",
    "- 문장: 에디가 책을 읽었다.\n",
    "  - 띄어쓰기 단위 토큰화: ['에디가', '책을', '읽었다']\n",
    "  - 형태소 단위 토큰화\n",
    "    - 자립 형태소: 에디, 책\n",
    "    - 의존 형태소: -가, -을, 읽-, -었, -다\n",
    "    \n",
    "### (2) 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않다.\n",
    "- 여러 견해가 있으나 한국어가 띄어쓰기가 없어도 잘 읽어지는 언어라는 견해도 있다.\n",
    "\n",
    "\n",
    "## 6) 품사 태깅(Part-of-speech tagging)\n",
    "- 단어는 표기는 같지만 품사에 따라서 의미가 달라지기도 한다.\n",
    "  - 못: 명사로는 망치를 사용해서 목재 따위를 고정하는 물건, 부사로서는 동작 동사를 할 수 없다는 의미로 사용\n",
    "- 코엔엘파이를 통해서 사용할 수 있는 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39724b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 : ['I', 'am', 'actively', 'looking', 'for', 'Ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'Ph.D.', 'student', '.'] \n",
      "\n",
      "품사 태깅 : [('I', 'PRP'), ('am', 'VBP'), ('actively', 'RB'), ('looking', 'VBG'), ('for', 'IN'), ('Ph.D.', 'NNP'), ('students', 'NNS'), ('.', '.'), ('and', 'CC'), ('you', 'PRP'), ('are', 'VBP'), ('a', 'DT'), ('Ph.D.', 'NNP'), ('student', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n영어 문장에 대해서 토큰화를 수행한 결과를 입력으로 품사 태깅을 수행하였습니다. \\nPenn Treebank POG Tags에서 PRP는 인칭 대명사, VBP는 동사, RB는 부사, VBG는 현재부사, IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사를 의미합니다.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print('단어 토큰화 :',tokenized_sentence, '\\n')\n",
    "print('품사 태깅 :',pos_tag(tokenized_sentence))\n",
    "\n",
    "\"\"\"\n",
    "영어 문장에 대해서 토큰화를 수행한 결과를 입력으로 품사 태깅을 수행하였습니다. \n",
    "Penn Treebank POG Tags에서 PRP는 인칭 대명사, VBP는 동사, RB는 부사, VBG는 현재부사, \n",
    "IN은 전치사, NNP는 고유 명사, NNS는 복수형 명사, CC는 접속사, DT는 관사를 의미합니다.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be8277a4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKT 형태소 분석 : ['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요'] \n",
      "\n",
      "OKT 품사 태깅 : [('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')] \n",
      "\n",
      "OKT 명사 추출 : ['코딩', '당신', '연휴', '여행']\n",
      "------------------------------\n",
      "꼬꼬마 형태소 분석 : ['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요'] \n",
      "\n",
      "꼬꼬마 품사 태깅 : [('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')] \n",
      "\n",
      "꼬꼬마 명사 추출 : ['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "\n",
    "print('OKT 형태소 분석 :',okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"), '\\n')\n",
    "print('OKT 품사 태깅 :',okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"), '\\n')\n",
    "print('OKT 명사 추출 :',okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print('-'* 30)\n",
    "print('꼬꼬마 형태소 분석 :',kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"), '\\n')\n",
    "print('꼬꼬마 품사 태깅 :',kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"), '\\n')\n",
    "print('꼬꼬마 명사 추출 :',kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86753f87",
   "metadata": {},
   "source": [
    "- 코엔엘 파이에서 제공하는 형태소 분석기들은 공통적으로 아래의 메소드를 제공한다.\n",
    "  - morphs: 형태소 추출\n",
    "  - pos: 품사태깅(part-of-speech tagging)\n",
    "  - nouns: 명사 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11680740",
   "metadata": {},
   "source": [
    "# 요약\n",
    "- 텍스트 전처리는 토큰화, 정제, 정규화로 나뉜다.\n",
    "- 토큰화\n",
    "  - 단어 토큰화\n",
    "  - 문장 토큰화\n",
    "- 토큰화는 의미있는 단위로 코퍼스(말뭉치)를 나눠 주는 것이다.\n",
    "- 한국어 토큰화(교착어, 조사 및 어미가 있는 언어)\n",
    "  - 형태소 토큰화(자립 형태소, 의존 형태소)\n",
    "  - 품사 태깅\n",
    "  - 명사 추출"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
